# Phase 6 Day 1: LLM-based RAGAS - ì™„ë£Œ ë¦¬í¬íŠ¸

**ì¼ì:** 2025-10-11
**ëª©í‘œ:** LLM-based RAGAS Evaluator êµ¬í˜„ (P0 ìš°ì„ ìˆœìœ„)

---

## ğŸ¯ Day 1 ëª©í‘œ ë‹¬ì„±

| ì‘ì—… | ìƒíƒœ | ì‚°ì¶œë¬¼ |
|------|------|--------|
| **PHASE_6_START.md ë¡œë“œë§µ** | âœ… ì™„ë£Œ | `PHASE_6_START.md` |
| **LLM RAGAS Types ì •ì˜** | âœ… ì™„ë£Œ | `src/evaluation/ragas/llm-ragas-types.ts` |
| **LLM RAGAS Evaluator êµ¬í˜„** | âœ… ì™„ë£Œ | `src/evaluation/ragas/llm-ragas-evaluator.ts` |
| **LLM RAGAS ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸** | â³ Day 2 | `scripts/run-llm-ragas-benchmark.ts` |

---

## ğŸ“‹ êµ¬í˜„ ì™„ë£Œ ë‚´ì—­

### 1ï¸âƒ£ PHASE_6_START.md ë¡œë“œë§µ (âœ… ì™„ë£Œ)

**ë‚´ìš©:**
- Phase 6 ì „ì²´ ê³„íš (2ì£¼, 7ê°€ì§€ ì•¡ì…˜)
- ì”ì—¬ ë¦¬ìŠ¤í¬ 4ê°€ì§€ ì •ë°€ ë¶„ì„
- Go/No-Go ê¸°ì¤€ ëª…í™•í™”
- ê²€ì¦ ì ˆì°¨ ìë™í™” ê³„íš

**í•µì‹¬ ëª©í‘œ:**
- Gate B/D/E: 40% â†’ 70-90%
- OOV Pass Rate: 0% â†’ 50%+
- Adversarial Overall: 75% â†’ 80%+

**íŒŒì¼:** `PHASE_6_START.md`

---

### 2ï¸âƒ£ LLM RAGAS Types ì •ì˜ (âœ… ì™„ë£Œ)

**êµ¬í˜„ ë‚´ìš©:**

#### íƒ€ì… ì •ì˜
```typescript
// LLM Provider
type LLMProvider = 'openai' | 'anthropic';

// Configuration
interface LLMRAGASConfig {
  provider: LLMProvider;
  model: string;
  apiKey: string;
  samplingRate: number;    // 0.2 = 20%
  batchSize: number;       // 5 (parallel)
  timeout: number;         // 30s
  temperature: number;     // 0.0 (deterministic)
  maxTokens: number;       // 500
  enableCache: boolean;    // true
  cacheTTL: number;        // 1 hour
}

// Input (same as RAGAS)
interface LLMRAGASInput {
  question: string;
  answer: string;
  contexts: string[];
  groundTruth: string;
}

// Output (with LLM reasoning)
interface LLMRAGASOutput {
  contextRecall: number;        // Gate B
  contextPrecision: number;     // Gate D
  answerRelevance: number;      // Gate E
  answerFaithfulness: number;   // Gate G
  overall: number;              // Geometric mean
  reasoning: {
    contextRecall: string;
    contextPrecision: string;
    answerRelevance: string;
    answerFaithfulness: string;
  };
  cost: {
    tokens: number;
    costUSD: number;
  };
  latencyMs: number;
}
```

#### Gate Thresholds
```typescript
const LLM_RAGAS_GATE_THRESHOLDS = {
  contextRecall: 0.7,      // Gate B
  contextPrecision: 0.75,  // Gate D
  answerRelevance: 0.85,   // Gate E
  answerFaithfulness: 0.9, // Gate G
};
```

#### Prompt Templates
- **Context Recall:** "Does the context cover all key information in groundTruth?"
- **Context Precision:** "Are all contexts relevant to the question?"
- **Answer Relevance:** "Does the answer directly address the question?"
- **Answer Faithfulness:** "Is the answer grounded in the contexts?"

**íŠ¹ì§•:**
- JSON ì¶œë ¥ ê°•ì œ (structured output)
- 1-2 ë¬¸ì¥ reasoning (ë””ë²„ê¹…ìš©)
- Token/ë¹„ìš© ì¶”ì 

**íŒŒì¼:** `src/evaluation/ragas/llm-ragas-types.ts`

---

### 3ï¸âƒ£ LLM RAGAS Evaluator êµ¬í˜„ (âœ… ì™„ë£Œ)

**êµ¬ì¡°:**

```typescript
class LLMRAGASEvaluator {
  // Single evaluation
  async evaluate(input: LLMRAGASInput): Promise<LLMRAGASResult>

  // Batch evaluation with sampling
  async evaluateBatch(
    inputs: LLMRAGASInput[],
    samplingRate?: number
  ): Promise<{
    results: LLMRAGASResult[];
    summary: LLMRAGASSummary;
  }>

  // Private methods
  private evaluateContextRecall(input): Promise<...>
  private evaluateContextPrecision(input): Promise<...>
  private evaluateAnswerRelevance(input): Promise<...>
  private evaluateAnswerFaithfulness(input): Promise<...>

  private callLLM(prompt): Promise<...>
  private callAnthropic(prompt): Promise<...>
  private callOpenAI(prompt): Promise<...>
}
```

**í•µì‹¬ ê¸°ëŠ¥:**

1. **Dual Provider Support**
   - Anthropic Claude 3.5 Sonnet (primary)
     - Input: $3/M tokens
     - Output: $15/M tokens
   - OpenAI GPT-4 Turbo (fallback)
     - Input: $10/M tokens
     - Output: $30/M tokens

2. **20% Sampling**
   - 100 queries â†’ 20 queries evaluated
   - Random sampling (shuffled)
   - Cost: ~$5-10 (Claude) vs $15-30 (GPT-4)

3. **Batch Processing**
   - Parallel evaluation (batch size: 5)
   - Rate limiting ì¤€ìˆ˜
   - Progress logging

4. **Caching**
   - In-memory cache (1 hour TTL)
   - Cache key: JSON.stringify(input)
   - Cost/latency ì ˆê°

5. **Prompt Engineering**
   - Structured JSON output
   - Clear instructions
   - Score normalization (0.0-1.0)

**ì˜ˆìƒ ì„±ëŠ¥:**

| ë©”íŠ¸ë¦­ | ì˜ˆìƒ ê°’ |
|--------|---------|
| **Context Recall** | 70-80% (vs 40% heuristic) |
| **Context Precision** | 75-85% (vs 40% heuristic) |
| **Answer Relevance** | 85-90% (vs 40% heuristic) |
| **Answer Faithfulness** | 90-95% (vs 100% heuristic) |

**ë¹„ìš© ì˜ˆì‚°:**
- 100 queries Ã— 20% sampling = 20 queries
- 4 metrics Ã— 20 queries = 80 LLM calls
- ~500 tokens/call Ã— 80 = 40,000 tokens
- Claude: $3 (input) + $15 (output) Ã— 40K/1M = **$0.72**
- GPT-4: $10 (input) + $30 (output) Ã— 40K/1M = **$1.60**

**íŒŒì¼:** `src/evaluation/ragas/llm-ragas-evaluator.ts`

---

## ğŸ“Š ì§„í–‰ ìƒí™©

### âœ… ì™„ë£Œ (Day 1)
- [x] PHASE_6_START.md ë¡œë“œë§µ ì‘ì„±
- [x] LLM RAGAS Types ì •ì˜
- [x] LLM RAGAS Evaluator êµ¬í˜„ (Anthropic + OpenAI)

### â³ ë‹¤ìŒ (Day 2)
- [ ] LLM RAGAS ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] LLM RAGAS ì‹¤í–‰ ë° ê²€ì¦
- [ ] Gate B/D/E ê°œì„  í™•ì¸

### ğŸ“… Week 1 ë‚¨ì€ ì‘ì—…
- Day 3: IR Metrics êµ¬í˜„
- Day 4: RRF ê·¸ë¦¬ë“œì„œì¹˜ ìŠ¤í¬ë¦½íŠ¸
- Day 5: RRF ìµœì  ì„¤ì • ë„ì¶œ

---

## ğŸ¯ ì˜ˆìƒ ê²°ê³¼

### Gate Pass Rates (Before â†’ After)

| Gate | Heuristic (Phase 5) | LLM-based (Phase 6 ëª©í‘œ) | ê°œì„  |
|------|-------------------|------------------------|------|
| **B (Recall)** | 40% | **70-80%** | +30-40pp |
| **D (Precision)** | 40% | **75-85%** | +35-45pp |
| **E (Relevance)** | 40% | **85-90%** | +45-50pp |
| **G (Faithfulness)** | 100% | **90-95%** | -5-10pp (ì •í™•ë„ í–¥ìƒ) |

### ë¹„ìš© ë¶„ì„

| í•­ëª© | Heuristic | LLM-based |
|------|-----------|-----------|
| **API ë¹„ìš©** | $0 | **$0.72-1.60** (20% sampling) |
| **ì •í™•ë„** | 40% | **70-90%** |
| **ì‹ ë¢°ë„** | ë‚®ìŒ (ê·œì¹™ ê¸°ë°˜) | **ë†’ìŒ (LLM íŒë‹¨)** |

---

## ğŸš€ ì‚¬ìš© ë°©ë²• (Day 2 ì˜ˆì •)

### 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```bash
# Anthropic (ê¶Œì¥)
export ANTHROPIC_API_KEY=your-api-key

# OpenAI (fallback)
export OPENAI_API_KEY=your-api-key
```

### 2. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (Day 2)
```bash
# 20% sampling (default)
npx tsx scripts/run-llm-ragas-benchmark.ts

# 100% (full evaluation)
LLM_RAGAS_SAMPLING_RATE=1.0 npx tsx scripts/run-llm-ragas-benchmark.ts

# OpenAI ì‚¬ìš©
LLM_RAGAS_PROVIDER=openai npx tsx scripts/run-llm-ragas-benchmark.ts
```

### 3. ê²°ê³¼ í™•ì¸
```bash
# ë¦¬í¬íŠ¸
cat reports/ragas/llm-ragas-phase6.json

# Gate ë¹„êµ
jq '.summary.gatePassRates' reports/ragas/llm-ragas-phase6.json
```

---

## ğŸ” ê¸°ìˆ ì  í•˜ì´ë¼ì´íŠ¸

### 1. Prompt Engineering
- **ëª…í™•í•œ ì§€ì‹œ:** "Return a score between 0.0 and 1.0"
- **JSON ê°•ì œ:** Structured output (OpenAI) / Parse (Anthropic)
- **ê°„ê²°í•œ reasoning:** 1-2 ë¬¸ì¥ (ë””ë²„ê¹…ìš©)

### 2. Cost Optimization
- **20% Sampling:** ë¹„ìš© 80% ì ˆê°
- **Caching:** ì¤‘ë³µ í‰ê°€ ë°©ì§€
- **Batch Processing:** Rate limit ì¤€ìˆ˜

### 3. Flexibility
- **Dual Provider:** Anthropic (primary) + OpenAI (fallback)
- **Configurable:** Sampling rate, batch size, timeout
- **Compatible:** ê¸°ì¡´ RAGASì™€ ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜

---

## ğŸ“ í•™ìŠµ ë‚´ìš©

### LLM-Judge íŒ¨í„´
- LLMì„ í‰ê°€ìë¡œ ì‚¬ìš© (Meta-evaluation)
- Prompt engineeringì˜ ì¤‘ìš”ì„±
- Structured output (JSON) ê°•ì œ

### Cost Management
- Sampling ì „ëµ (20% â†’ 80% ë¹„ìš© ì ˆê°)
- Cachingìœ¼ë¡œ ì¤‘ë³µ ë°©ì§€
- Provider ì„ íƒ (Claude < GPT-4)

### Evaluation Quality
- Heuristicì˜ í•œê³„ (ê·œì¹™ ê¸°ë°˜)
- LLMì˜ ì¥ì  (ë§¥ë½ ì´í•´, ì¶”ë¡ )
- Trade-off (ë¹„ìš© vs ì •í™•ë„)

---

## âš ï¸ ì£¼ì˜ ì‚¬í•­

### API Key í•„ìˆ˜
```bash
# Anthropic (ê¶Œì¥)
export ANTHROPIC_API_KEY=sk-ant-...

# OpenAI (fallback)
export OPENAI_API_KEY=sk-...
```

### Rate Limits
- Anthropic: 50 requests/min (Tier 1)
- OpenAI: 500 requests/min (Tier 1)
- Batch size: 5 (safe)

### Cost Monitoring
- 20% sampling: ~$0.72 (Claude) / ~$1.60 (GPT-4)
- 100% sampling: ~$3.60 (Claude) / ~$8.00 (GPT-4)
- ì˜ˆì‚°: $10/day (ì¶©ë¶„)

---

## ğŸ“ ì‚°ì¶œë¬¼

### ì½”ë“œ
- âœ… `src/evaluation/ragas/llm-ragas-types.ts` (205 lines)
- âœ… `src/evaluation/ragas/llm-ragas-evaluator.ts` (450 lines)

### ë¬¸ì„œ
- âœ… `PHASE_6_START.md` (ì „ì²´ ë¡œë“œë§µ)
- âœ… `PHASE_6_DAY_1_COMPLETE.md` (ì´ ë¦¬í¬íŠ¸)

### ë‹¤ìŒ Day
- â³ `scripts/run-llm-ragas-benchmark.ts` (Day 2)
- â³ `reports/ragas/llm-ragas-phase6.json` (Day 2)

---

## ğŸš€ Day 2 ê³„íš

### ëª©í‘œ
- LLM RAGAS ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- Real Hybrid Benchmark ë°ì´í„°ë¡œ ì‹¤í–‰
- Gate B/D/E ê°œì„  í™•ì¸

### ì˜ˆìƒ ì‹œê°„
- ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±: 1ì‹œê°„
- ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰: 30ë¶„ (20% sampling)
- ê²°ê³¼ ë¶„ì„: 30ë¶„

### ì„±ê³µ ê¸°ì¤€
- âœ… Gate B â‰¥ 70%
- âœ… Gate D â‰¥ 75%
- âœ… Gate E â‰¥ 85%
- âœ… ë¹„ìš© < $2

---

**ì‘ì„±ì:** Claude Code
**ì¼ì:** 2025-10-11
**Phase:** 6 Day 1
**ëª©í‘œ:** LLM-based RAGAS Evaluator êµ¬í˜„ âœ… ì™„ë£Œ
