# RAGAS Evaluation Framework (Phase 4)

**Status:** ğŸš§ Foundation Created (Week 2) â†’ Phase 4 Implementation Pending

---

## What is RAGAS?

**RAGAS** (Retrieval-Augmented Generation Assessment) is a framework for evaluating RAG systems using LLM-as-a-Judge.

**Key Metrics:**
1. **Context Recall** - Did retrieval get all necessary information?
2. **Context Precision** - Are retrieved contexts relevant?
3. **Answer Faithfulness** - Is answer grounded in retrieved contexts?
4. **Answer Relevance** - Does answer match question intent?

---

## Why RAGAS?

### Problem: Manual QA Evaluation is Expensive

**Traditional Approach:**
- âŒ Human reviewers rate each Q&A pair
- âŒ Time: ~5 min/pair Ã— 1000 pairs = 83 hours
- âŒ Cost: $25/hour Ã— 83 hours = $2,075
- âŒ Consistency: Inter-rater agreement ~70%

**RAGAS Approach:**
- âœ… LLM evaluates using structured prompts
- âœ… Time: ~5 sec/pair Ã— 1000 pairs = 1.4 hours
- âœ… Cost: $0.01/pair Ã— 1000 pairs = $10
- âœ… Consistency: Deterministic (same prompt â†’ same score)

---

## RAGAS â†” Gate Mapping

Our existing Gate system maps perfectly to RAGAS metrics:

| RAGAS Metric | Maps to Gate | What it Measures |
|--------------|--------------|-------------------|
| **Context Recall** | Gate B (Evidence Hit) | Retrieval coverage |
| **Context Precision** | Gate D (Diversity) | Retrieval relevance |
| **Answer Faithfulness** | Gate G (Compliance) | Grounding in evidence |
| **Answer Relevance** | Gate E (Explanation) | Answer quality |

**Benefit:** We can auto-generate Gate scores from RAGAS metrics!

---

## How RAGAS Works

### Input
```typescript
{
  question: "ì•„ì´ëŒë´„ ì„œë¹„ìŠ¤ ìš”ê¸ˆì€ ì–¼ë§ˆì¸ê°€ìš”?",
  answer: "ê¸°ë³¸í˜•ì€ 11,630ì›, ì¢…í•©í˜•ì€ 15,110ì›ì…ë‹ˆë‹¤.",
  contexts: [
    "ì œ3ì¡° ì•„ì´ëŒë´„ ì„œë¹„ìŠ¤ ìš”ê¸ˆ\nê¸°ë³¸í˜• 11,630ì›\nì¢…í•©í˜• 15,110ì›",
    "ì •ë¶€ ì§€ì›ê¸ˆì€ ì†Œë“ì— ë”°ë¼ ì°¨ë“± ì§€ê¸‰ë©ë‹ˆë‹¤."
  ],
  groundTruth: "ê¸°ë³¸í˜• 11,630ì›, ì¢…í•©í˜• 15,110ì›" // Optional
}
```

### Process

1. **Context Recall** (LLM Evaluation)
   ```
   Prompt: "Based on the ground truth answer, what % of necessary
           information is present in the retrieved contexts?"
   Output: 100% (all pricing info retrieved)
   Score: 1.0
   ```

2. **Context Precision** (LLM Evaluation)
   ```
   Prompt: "For each retrieved context, is it relevant to answering
           the question?"
   Output:
     - Context 1: Relevant (pricing)
     - Context 2: Irrelevant (support, not pricing)
   Score: 0.5 (1/2 contexts relevant)
   ```

3. **Answer Faithfulness** (LLM Evaluation)
   ```
   Prompt: "Break down the answer into statements. For each statement,
           is it supported by the contexts?"
   Output:
     - "ê¸°ë³¸í˜•ì€ 11,630ì›" â†’ Supported (Context 1)
     - "ì¢…í•©í˜•ì€ 15,110ì›" â†’ Supported (Context 1)
   Score: 1.0 (2/2 statements grounded)
   ```

4. **Answer Relevance** (Embedding Similarity)
   ```
   Compute: cosine_similarity(embed(question), embed(answer))
   Score: 0.92 (highly relevant)
   ```

### Output
```typescript
{
  metrics: {
    contextRecall: 1.0,
    contextPrecision: 0.5,
    answerFaithfulness: 1.0,
    answerRelevance: 0.92,
    overall: 0.82 // Harmonic mean
  },
  gateMapping: {
    contextRecall: "Gate B: 1.0",
    contextPrecision: "Gate D: 0.5",
    answerFaithfulness: "Gate G: 1.0",
    answerRelevance: "Gate E: 0.92"
  }
}
```

---

## Expected Performance (RFC Targets)

| Metric | Baseline | Phase 3 (Vision+Hybrid) | Phase 4 (RAGAS) |
|--------|----------|------------------------|-----------------|
| Context Recall | 65% | 85% | **88%** âœ… |
| Context Precision | 70% | 85% | **87%** âœ… |
| Faithfulness | 80% | 90% | **92%** âœ… |
| Relevance | 75% | 88% | **90%** âœ… |

---

## Implementation Plan

### Phase 4 Week 1: Core RAGAS
1. â³ Implement `RAGASEvaluator` class
2. â³ Add LLM-based Context Recall evaluator
3. â³ Add LLM-based Context Precision evaluator
4. â³ Add LLM-based Answer Faithfulness evaluator
5. â³ Add embedding-based Answer Relevance

### Phase 4 Week 2: Gate Integration
1. â³ Auto-map RAGAS â†’ Gate scores
2. â³ Update Gate reports with RAGAS metrics
3. â³ Add RAGAS to CI/CD pipeline

### Phase 4 Week 3: Automation
1. â³ Batch evaluation script
2. â³ Scheduled RAGAS runs (nightly)
3. â³ Quality regression detection

---

## Usage Example (Phase 4 Target)

```typescript
import { RAGASEvaluator } from './ragas-evaluator';

const evaluator = new RAGASEvaluator({
  llmProvider: 'anthropic',
  model: 'claude-3-5-sonnet-20241022',
  apiKey: process.env.ANTHROPIC_API_KEY,
  saveResults: true
});

// Single evaluation
const result = await evaluator.evaluate({
  question: "ì•„ì´ëŒë´„ ì„œë¹„ìŠ¤ ìš”ê¸ˆì€ ì–¼ë§ˆì¸ê°€ìš”?",
  answer: "ê¸°ë³¸í˜•ì€ 11,630ì›, ì¢…í•©í˜•ì€ 15,110ì›ì…ë‹ˆë‹¤.",
  contexts: [
    "ì œ3ì¡° ì•„ì´ëŒë´„ ì„œë¹„ìŠ¤ ìš”ê¸ˆ\nê¸°ë³¸í˜• 11,630ì›\nì¢…í•©í˜• 15,110ì›"
  ]
});

console.log(result.metrics.overall); // 0.92

// Batch evaluation
const dataset = loadQAPairs('datasets/qa-test.json');
const results = await evaluator.evaluateBatch(dataset);

// Save report
await evaluator.saveReport(results, 'reports/ragas/phase4.json');
```

---

## Files to Create (Phase 4)

- [ ] `ragas-evaluator.ts` - Main evaluator
- [ ] `metrics/context-recall.ts` - Context Recall calculator
- [ ] `metrics/context-precision.ts` - Context Precision calculator
- [ ] `metrics/answer-faithfulness.ts` - Faithfulness calculator
- [ ] `metrics/answer-relevance.ts` - Relevance calculator
- [ ] `gate-integration.ts` - Map RAGAS â†’ Gates
- [ ] `types.ts` - âœ… Already created
- [ ] `README.md` - âœ… This file

---

## References

- **RAGAS Paper:** https://arxiv.org/abs/2309.15217
- **RFC:** `designs/rfc/rfc-integrate-multimodal-rag-augmentation.md`
- **Gate System:** `src/core/quality/gating-system.ts`
- **Evaluation Guide:** `docs/EVALUATION_GUIDE.md`

---

**Next Session Command:**
```bash
# Phase 4: Start RAGAS implementation
# 1. Install dependencies
npm install @langchain/community @langchain/core

# 2. Implement evaluator
code src/evaluation/ragas/ragas-evaluator.ts

# 3. Run tests
npm run test -- ragas
```

---

**Status:** âœ… Foundation complete, ready for Phase 4 implementation
